{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils\n",
    "import transformer\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set configuration\n",
    "STYLE_MODELS_DIR = \"transforms/\"\n",
    "VIDEOS_INPUT_DIR = \"videos/input\"\n",
    "VIDEOS_OUTPUT_DIR = \"videos/output\"\n",
    "TEMPORAL_WEIGHT = 0.5  # Weight for temporal consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(directory):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def load_image_from_array(img_array):\n",
    "    \"\"\"Load PIL image from numpy array\"\"\"\n",
    "    return Image.fromarray(img_array)\n",
    "\n",
    "class ObjectDetector:\n",
    "    def __init__(self, device=None):\n",
    "        \"\"\"Initialize object detector model\"\"\"\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device for object detection: {self.device}\")\n",
    "        \n",
    "        # Load pre-trained model\n",
    "        print(\"Loading object detection model...\")\n",
    "        self.model = models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Define transformation\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # COCO class names\n",
    "        self.class_names = [\n",
    "            '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "            'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "            'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "            'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "            'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "            'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "            'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "            'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "        ]\n",
    "    \n",
    "    def detect_objects(self, frame):\n",
    "        \"\"\"\n",
    "        Detect objects in a frame\n",
    "        \n",
    "        Args:\n",
    "            frame: OpenCV BGR frame\n",
    "            \n",
    "        Returns:\n",
    "            List of detected objects with boxes, masks, and class info\n",
    "        \"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        img_tensor = self.transform(rgb_frame).to(self.device)\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model([img_tensor])\n",
    "        \n",
    "        # Process results\n",
    "        objects = []\n",
    "        for i in range(len(prediction[0]['boxes'])):\n",
    "            score = prediction[0]['scores'][i].cpu().item()\n",
    "            \n",
    "            # Filter by confidence\n",
    "            if score > 0.5:\n",
    "                box = prediction[0]['boxes'][i].cpu().numpy().astype(np.int32)\n",
    "                label_id = prediction[0]['labels'][i].cpu().item()\n",
    "                label = self.class_names[label_id]\n",
    "                \n",
    "                # Get mask if available\n",
    "                mask = None\n",
    "                if 'masks' in prediction[0]:\n",
    "                    if len(prediction[0]['masks']) > i:\n",
    "                        mask = prediction[0]['masks'][i, 0].cpu().numpy() > 0.5\n",
    "                \n",
    "                objects.append({\n",
    "                    'box': box,\n",
    "                    'label': label,\n",
    "                    'score': score,\n",
    "                    'mask': mask\n",
    "                })\n",
    "        \n",
    "        return objects\n",
    "\n",
    "class RegionBasedStyleTransfer:\n",
    "    def __init__(self, style_foreground_path, style_background_path, device=None, \n",
    "                preserve_color_fg=False, preserve_color_bg=False):\n",
    "        \"\"\"Initialize the region-based style transfer model\"\"\"\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device for style transfer: {self.device}\")\n",
    "        \n",
    "        # Load transformer networks\n",
    "        if style_foreground_path is not None:\n",
    "            print(\"Loading foreground style model...\")\n",
    "            self.net_fg = transformer.TransformerNetwork()\n",
    "            self.net_fg.load_state_dict(torch.load(style_foreground_path))\n",
    "            self.net_fg = self.net_fg.to(self.device)\n",
    "            self.net_fg.eval()\n",
    "        else:\n",
    "            print(\"Foreground will use original pixels (no style)\")\n",
    "            self.net_fg = None\n",
    "        \n",
    "        if style_background_path is not None:\n",
    "            print(\"Loading background style model...\")\n",
    "            self.net_bg = transformer.TransformerNetwork()\n",
    "            self.net_bg.load_state_dict(torch.load(style_background_path))\n",
    "            self.net_bg = self.net_bg.to(self.device)\n",
    "            self.net_bg.eval()\n",
    "        else:\n",
    "            print(\"Background will use original pixels (no style)\")\n",
    "            self.net_bg = None\n",
    "        \n",
    "        # Initialize object detector\n",
    "        self.detector = ObjectDetector(device)\n",
    "        \n",
    "        # Style settings\n",
    "        self.preserve_color_fg = preserve_color_fg\n",
    "        self.preserve_color_bg = preserve_color_bg\n",
    "        \n",
    "        # Temporal consistency\n",
    "        self.prev_stylized_fg = None\n",
    "        self.prev_stylized_bg = None\n",
    "    \n",
    "    def stylize_region(self, frame, is_foreground=True, apply_temporal=True):\n",
    "        \"\"\"\n",
    "        Apply style transfer to a region of a frame\n",
    "        \n",
    "        Args:\n",
    "            frame: OpenCV frame in BGR format\n",
    "            is_foreground: Whether to use foreground or background style\n",
    "            apply_temporal: Whether to apply temporal consistency\n",
    "            \n",
    "        Returns:\n",
    "            Stylized frame in BGR format\n",
    "        \"\"\"\n",
    "        # Select appropriate network and settings\n",
    "        if is_foreground:\n",
    "            net = self.net_fg\n",
    "            prev_stylized = self.prev_stylized_fg\n",
    "            preserve_color = self.preserve_color_fg\n",
    "        else:\n",
    "            net = self.net_bg\n",
    "            prev_stylized = self.prev_stylized_bg\n",
    "            preserve_color = self.preserve_color_bg\n",
    "            \n",
    "        # Check if the corresponding network exists\n",
    "        if net is None:\n",
    "            return frame  # Return original frame if no style network\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        content_image = load_image_from_array(frame_rgb)\n",
    "        \n",
    "        # Apply style transfer\n",
    "        content_tensor = utils.itot(content_image).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            stylized_tensor = net(content_tensor)\n",
    "            \n",
    "            # Apply temporal consistency if needed\n",
    "            if apply_temporal and prev_stylized is not None:\n",
    "                stylized_tensor = (1 - TEMPORAL_WEIGHT) * stylized_tensor + TEMPORAL_WEIGHT * prev_stylized\n",
    "            \n",
    "            # Store current output for next frame\n",
    "            if is_foreground:\n",
    "                self.prev_stylized_fg = stylized_tensor.clone()\n",
    "            else:\n",
    "                self.prev_stylized_bg = stylized_tensor.clone()\n",
    "        \n",
    "        # Convert back to image\n",
    "        stylized_image = utils.ttoi(stylized_tensor.detach())\n",
    "        \n",
    "        if preserve_color:\n",
    "            stylized_image = utils.transfer_color(content_image, stylized_image)\n",
    "        \n",
    "        # Convert back to BGR for OpenCV\n",
    "        stylized_frame = cv2.cvtColor(np.array(stylized_image), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        return stylized_frame\n",
    "    \n",
    "    def process_frame(self, frame, apply_temporal=True, target_classes=None):\n",
    "        \"\"\"\n",
    "        Process a frame with region-based style transfer\n",
    "        \n",
    "        Args:\n",
    "            frame: OpenCV frame in BGR format\n",
    "            apply_temporal: Whether to apply temporal consistency\n",
    "            target_classes: List of classes to style as foreground (None for all)\n",
    "            \n",
    "        Returns:\n",
    "            Stylized frame in BGR format\n",
    "        \"\"\"\n",
    "        # Get original frame dimensions\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # Detect objects\n",
    "        objects = self.detector.detect_objects(frame)\n",
    "        \n",
    "        # Create foreground mask (initialize with zeros)\n",
    "        fg_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        \n",
    "        # Add detected objects to foreground mask if they match target classes\n",
    "        for obj in objects:\n",
    "            if target_classes is None or obj['label'] in target_classes:\n",
    "                if obj['mask'] is not None:\n",
    "                    # Use object mask\n",
    "                    fg_mask = np.logical_or(fg_mask, obj['mask']).astype(np.uint8) * 255\n",
    "                else:\n",
    "                    # Use bounding box\n",
    "                    x1, y1, x2, y2 = obj['box']\n",
    "                    cv2.rectangle(fg_mask, (x1, y1), (x2, y2), 255, -1)\n",
    "        \n",
    "        # Dilate the mask slightly to avoid hard edges\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        fg_mask = cv2.dilate(fg_mask, kernel, iterations=1)\n",
    "        \n",
    "        # Create background mask (inverse of foreground)\n",
    "        bg_mask = 255 - fg_mask\n",
    "        \n",
    "        # Convert masks to 3-channel and proper scale for blending\n",
    "        fg_mask_3c = cv2.cvtColor(fg_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "        bg_mask_3c = cv2.cvtColor(bg_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "        \n",
    "        # Start with the original frame\n",
    "        result = frame.copy()\n",
    "        \n",
    "        # Apply background style if provided\n",
    "        if self.net_bg is not None:\n",
    "            # Style the entire frame with background style\n",
    "            bg_stylized = self.stylize_region(frame, is_foreground=False, apply_temporal=apply_temporal)\n",
    "            # Only apply to background areas (where bg_mask is non-zero)\n",
    "            result = result * (1.0 - bg_mask_3c) + bg_stylized * bg_mask_3c\n",
    "        \n",
    "        # Apply foreground style if provided\n",
    "        if self.net_fg is not None:\n",
    "            # Style the entire frame with foreground style\n",
    "            fg_stylized = self.stylize_region(frame, is_foreground=True, apply_temporal=apply_temporal)\n",
    "            # Only apply to foreground areas (where fg_mask is non-zero)\n",
    "            result = result * (1.0 - fg_mask_3c) + fg_stylized * fg_mask_3c\n",
    "        \n",
    "        return result.astype(np.uint8)\n",
    "    \n",
    "    def process_video_headless(self, input_path, output_path, target_classes=None):\n",
    "        \"\"\"Process an entire video with region-based style transfer (no GUI)\"\"\"\n",
    "        # Disable OpenCV GUI\n",
    "        cv2.setUseOptimized(True)\n",
    "        os.environ[\"OPENCV_VIDEOIO_PRIORITY_BACKEND\"] = \"0\"\n",
    "        os.environ[\"OPENCV_VIDEOIO_DEBUG\"] = \"0\"\n",
    "        \n",
    "        # Open video file\n",
    "        video = cv2.VideoCapture(input_path)\n",
    "        if not video.isOpened():\n",
    "            print(f\"Error: Could not open video file {input_path}\")\n",
    "            return\n",
    "        \n",
    "        # Get video properties\n",
    "        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        print(f\"Video properties: {width}x{height}, {fps} fps, {total_frames} frames\")\n",
    "        \n",
    "        # Create video writer\n",
    "        ensure_dir(os.path.dirname(output_path))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        if not out.isOpened():\n",
    "            print(f\"Error: Could not create output video file {output_path}\")\n",
    "            video.release()\n",
    "            return\n",
    "        \n",
    "        frame_count = 0\n",
    "        processing_times = []\n",
    "        \n",
    "        print(\"Processing video...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_start = time.time()\n",
    "            \n",
    "            # Process frame with region-based style transfer\n",
    "            stylized_frame = self.process_frame(frame, apply_temporal=(frame_count > 0), \n",
    "                                             target_classes=target_classes)\n",
    "            \n",
    "            # Ensure frame is 8-bit unsigned integer format (CV_8U)\n",
    "            if stylized_frame.dtype != np.uint8:\n",
    "                stylized_frame = np.clip(stylized_frame, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # Write frame to output video\n",
    "            out.write(stylized_frame)\n",
    "            \n",
    "            # Calculate processing time\n",
    "            frame_time = time.time() - frame_start\n",
    "            processing_times.append(frame_time)\n",
    "            \n",
    "            # Show progress\n",
    "            frame_count += 1\n",
    "            if frame_count % 10 == 0 or frame_count == total_frames:\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_fps = frame_count / elapsed\n",
    "                estimated_total = elapsed / frame_count * total_frames\n",
    "                remaining = estimated_total - elapsed\n",
    "                \n",
    "                print(f\"Processed frame {frame_count}/{total_frames} - \"\n",
    "                      f\"Avg. FPS: {avg_fps:.2f} - \"\n",
    "                      f\"Avg. time per frame: {np.mean(processing_times[-10:]):.3f}s - \"\n",
    "                      f\"Remaining time: {remaining/60:.1f} min\")\n",
    "                \n",
    "                # Option to save intermediate frames (useful for debugging)\n",
    "                if frame_count % 100 == 0:\n",
    "                    # Save debug frames - uncommenting these would save debug images\n",
    "                    debug_dir = os.path.join(os.path.dirname(output_path), \"debug_frames\")\n",
    "                    ensure_dir(debug_dir)\n",
    "                    \n",
    "                    # Save current frame\n",
    "                    cv2.imwrite(os.path.join(debug_dir, f\"frame_{frame_count:06d}.jpg\"), frame)\n",
    "                    cv2.imwrite(os.path.join(debug_dir, f\"stylized_{frame_count:06d}.jpg\"), stylized_frame)\n",
    "        \n",
    "        # Release resources\n",
    "        video.release()\n",
    "        out.release()\n",
    "        \n",
    "        # Print stats\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Video processing completed in {total_time:.2f} seconds\")\n",
    "        print(f\"Average processing time per frame: {np.mean(processing_times):.3f} seconds\")\n",
    "        print(f\"Total average FPS: {frame_count / total_time:.2f}\")\n",
    "        print(f\"Stylized video saved to: {output_path}\")\n",
    "        \n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_based_style_transfer(input_path=None, output_path=None, \n",
    "                              fg_style_path=None, bg_style_path=None,\n",
    "                              target_classes=None, preserve_color_fg=False, \n",
    "                              preserve_color_bg=False):\n",
    "    \"\"\"\n",
    "    Apply region-based style transfer to a video (headless version for servers)\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input video\n",
    "        output_path: Path to save stylized video\n",
    "        fg_style_path: Path to foreground style model (None to preserve original objects)\n",
    "        bg_style_path: Path to background style model (None to preserve original background)\n",
    "        target_classes: List of object classes to treat as foreground\n",
    "        preserve_color_fg: Whether to preserve original colors for foreground\n",
    "        preserve_color_bg: Whether to preserve original colors for background\n",
    "    \"\"\"\n",
    "    # Set default paths if not provided\n",
    "    if input_path is None:\n",
    "        input_path = os.path.join(VIDEOS_INPUT_DIR, \"sample.mp4\")\n",
    "    \n",
    "    if output_path is None:\n",
    "        # Create output path based on input\n",
    "        video_name = os.path.basename(input_path)\n",
    "        output_path = os.path.join(VIDEOS_OUTPUT_DIR, f\"stylized_regions_{video_name}\")\n",
    "    \n",
    "    # Set default target classes if not provided\n",
    "    if target_classes is None:\n",
    "        # Default to people and pets as foreground\n",
    "        target_classes = ['person', 'dog', 'cat', 'horse']\n",
    "    \n",
    "    # Initialize model\n",
    "    model = RegionBasedStyleTransfer(\n",
    "        style_foreground_path=fg_style_path, \n",
    "        style_background_path=bg_style_path, \n",
    "        preserve_color_fg=preserve_color_fg,\n",
    "        preserve_color_bg=preserve_color_bg\n",
    "    )\n",
    "    \n",
    "    # Process video without display\n",
    "    return model.process_video_headless(input_path, output_path, target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for style transfer: cuda\n",
      "Foreground will use original pixels (no style)\n",
      "Loading background style model...\n",
      "Using device for object detection: cuda\n",
      "Loading object detection model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video properties: 1920x1080, 29.97002997002997 fps, 528 frames\n",
      "Processing video...\n",
      "Processed frame 10/528 - Avg. FPS: 2.13 - Avg. time per frame: 0.459s - Remaining time: 4.1 min\n",
      "Processed frame 20/528 - Avg. FPS: 2.40 - Avg. time per frame: 0.360s - Remaining time: 3.5 min\n",
      "Processed frame 30/528 - Avg. FPS: 2.52 - Avg. time per frame: 0.356s - Remaining time: 3.3 min\n",
      "Processed frame 40/528 - Avg. FPS: 2.59 - Avg. time per frame: 0.355s - Remaining time: 3.1 min\n",
      "Processed frame 50/528 - Avg. FPS: 2.64 - Avg. time per frame: 0.345s - Remaining time: 3.0 min\n",
      "Processed frame 60/528 - Avg. FPS: 2.68 - Avg. time per frame: 0.344s - Remaining time: 2.9 min\n",
      "Processed frame 70/528 - Avg. FPS: 2.70 - Avg. time per frame: 0.349s - Remaining time: 2.8 min\n",
      "Processed frame 80/528 - Avg. FPS: 2.73 - Avg. time per frame: 0.343s - Remaining time: 2.7 min\n",
      "Processed frame 90/528 - Avg. FPS: 2.73 - Avg. time per frame: 0.357s - Remaining time: 2.7 min\n",
      "Processed frame 100/528 - Avg. FPS: 2.73 - Avg. time per frame: 0.361s - Remaining time: 2.6 min\n",
      "Processed frame 110/528 - Avg. FPS: 2.74 - Avg. time per frame: 0.360s - Remaining time: 2.5 min\n",
      "Processed frame 120/528 - Avg. FPS: 2.74 - Avg. time per frame: 0.355s - Remaining time: 2.5 min\n",
      "Processed frame 130/528 - Avg. FPS: 2.75 - Avg. time per frame: 0.344s - Remaining time: 2.4 min\n",
      "Processed frame 140/528 - Avg. FPS: 2.76 - Avg. time per frame: 0.353s - Remaining time: 2.3 min\n",
      "Processed frame 150/528 - Avg. FPS: 2.76 - Avg. time per frame: 0.350s - Remaining time: 2.3 min\n",
      "Processed frame 160/528 - Avg. FPS: 2.77 - Avg. time per frame: 0.350s - Remaining time: 2.2 min\n",
      "Processed frame 170/528 - Avg. FPS: 2.77 - Avg. time per frame: 0.353s - Remaining time: 2.2 min\n",
      "Processed frame 180/528 - Avg. FPS: 2.78 - Avg. time per frame: 0.337s - Remaining time: 2.1 min\n",
      "Processed frame 190/528 - Avg. FPS: 2.79 - Avg. time per frame: 0.339s - Remaining time: 2.0 min\n",
      "Processed frame 200/528 - Avg. FPS: 2.79 - Avg. time per frame: 0.336s - Remaining time: 2.0 min\n",
      "Processed frame 210/528 - Avg. FPS: 2.80 - Avg. time per frame: 0.335s - Remaining time: 1.9 min\n",
      "Processed frame 220/528 - Avg. FPS: 2.81 - Avg. time per frame: 0.332s - Remaining time: 1.8 min\n",
      "Processed frame 230/528 - Avg. FPS: 2.82 - Avg. time per frame: 0.334s - Remaining time: 1.8 min\n",
      "Processed frame 240/528 - Avg. FPS: 2.82 - Avg. time per frame: 0.336s - Remaining time: 1.7 min\n",
      "Processed frame 250/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.341s - Remaining time: 1.6 min\n",
      "Processed frame 260/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.353s - Remaining time: 1.6 min\n",
      "Processed frame 270/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.336s - Remaining time: 1.5 min\n",
      "Processed frame 280/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.346s - Remaining time: 1.5 min\n",
      "Processed frame 290/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.349s - Remaining time: 1.4 min\n",
      "Processed frame 300/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.347s - Remaining time: 1.3 min\n",
      "Processed frame 310/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.356s - Remaining time: 1.3 min\n",
      "Processed frame 320/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.350s - Remaining time: 1.2 min\n",
      "Processed frame 330/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.360s - Remaining time: 1.2 min\n",
      "Processed frame 340/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.348s - Remaining time: 1.1 min\n",
      "Processed frame 350/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.353s - Remaining time: 1.0 min\n",
      "Processed frame 360/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.346s - Remaining time: 1.0 min\n",
      "Processed frame 370/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.346s - Remaining time: 0.9 min\n",
      "Processed frame 380/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.344s - Remaining time: 0.9 min\n",
      "Processed frame 390/528 - Avg. FPS: 2.84 - Avg. time per frame: 0.345s - Remaining time: 0.8 min\n",
      "Processed frame 400/528 - Avg. FPS: 2.84 - Avg. time per frame: 0.346s - Remaining time: 0.8 min\n",
      "Processed frame 410/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.358s - Remaining time: 0.7 min\n",
      "Processed frame 420/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.349s - Remaining time: 0.6 min\n",
      "Processed frame 430/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.351s - Remaining time: 0.6 min\n",
      "Processed frame 440/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.357s - Remaining time: 0.5 min\n",
      "Processed frame 450/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.355s - Remaining time: 0.5 min\n",
      "Processed frame 460/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.351s - Remaining time: 0.4 min\n",
      "Processed frame 470/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.358s - Remaining time: 0.3 min\n",
      "Processed frame 480/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.350s - Remaining time: 0.3 min\n",
      "Processed frame 490/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.347s - Remaining time: 0.2 min\n",
      "Processed frame 500/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.340s - Remaining time: 0.2 min\n",
      "Processed frame 510/528 - Avg. FPS: 2.83 - Avg. time per frame: 0.348s - Remaining time: 0.1 min\n",
      "Processed frame 520/528 - Avg. FPS: 2.84 - Avg. time per frame: 0.338s - Remaining time: 0.0 min\n",
      "Processed frame 528/528 - Avg. FPS: 2.84 - Avg. time per frame: 0.335s - Remaining time: 0.0 min\n",
      "Video processing completed in 186.03 seconds\n",
      "Average processing time per frame: 0.350 seconds\n",
      "Total average FPS: 2.84\n",
      "Stylized video saved to: videos/output/bg_only_stylized_ship2.mp4\n",
      "Stylized video saved to: videos/output/bg_only_stylized_ship2.mp4\n"
     ]
    }
   ],
   "source": [
    "# Example usage with style only for background\n",
    "result = region_based_style_transfer(\n",
    "    input_path=\"videos/input/ship.mp4\",  # Change to your video file\n",
    "    output_path=\"videos/output/bg_only_stylized_ship2.mp4\",\n",
    "    fg_style_path=None,  # No style for foreground (keep original)\n",
    "    bg_style_path=\"transforms/wave.pth\",  # Style for background\n",
    "    target_classes=['boat'],  # Objects to detect as foreground\n",
    "    preserve_color_fg=False,\n",
    "    preserve_color_bg=False\n",
    ")\n",
    "\n",
    "print(f\"Stylized video saved to: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
